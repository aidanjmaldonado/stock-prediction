{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidanjmaldonado/penny-stock-lstm/blob/main/penny_stock.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {
        "id": "AB9VHvt_xNXa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "import sqlite3\n",
        "import requests\n",
        "import sys\n",
        "from library.DataSetProcessor import DataSetProcessor\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Constants\n",
        "SEQUENCE_LENGTH = 78 # 1 day long\n",
        "PREDICTION_LENGTH = 78 # 1 day long\n",
        "NUM_FEATURES = 2 # close, volume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJBjdbHzoWvZ"
      },
      "source": [
        "# Create database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DGRIE9XnRw8",
        "outputId": "c53b5aec-8331-490b-b5b8-37e403b087c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Request to download database succeeded\n"
          ]
        }
      ],
      "source": [
        "# Download database from github repository\n",
        "historical_url = \"https://raw.githubusercontent.com/CSE-115-UCSC/penny-stock-lstm/main/historicaldata.db\"\n",
        "scrape_request = requests.get(historical_url)\n",
        "\n",
        "try:\n",
        "  # Contingent on request status\n",
        "  scrape_request = requests.get(historical_url)\n",
        "  scrape_request.raise_for_status()\n",
        "\n",
        "  # Create local database from pull, name 'historicaldata.db'\n",
        "  with open(\"historical.db\", \"wb\") as db_file:\n",
        "    db_file.write(scrape_request.content)\n",
        "    \n",
        "  print(\"Request to download database succeeded\")\n",
        "\n",
        "\n",
        "except:\n",
        "  # Report failed request status\n",
        "  sys.stderr.write(\"Request to download database failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KApKP8mve16y",
        "outputId": "eac13848-bd1a-4869-8752-8d266eaab896"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SQlite connected with historical.db\n"
          ]
        }
      ],
      "source": [
        "# Connect to SQlite database\n",
        "try:\n",
        "    db = 'historical.db'\n",
        "    sqliteConnection = sqlite3.connect(db)\n",
        "    cursor = sqliteConnection.cursor()\n",
        "    print(f'SQlite connected with {db}')\n",
        "\n",
        "except:\n",
        "    # Report failed request status\n",
        "    sys.stderr.write(\"Failed to connect to database\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or4POn4Ye_Lw",
        "outputId": "a222f695-d83c-4f79-bc32-348cb0749353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success querying all historical\n"
          ]
        }
      ],
      "source": [
        "# Query all historical stock data from Database\n",
        "try:\n",
        "    query = f\"SELECT * FROM all_historical;\"\n",
        "    cursor.execute(query)\n",
        "    if cursor.fetchone() is None:\n",
        "        raise Exception(\"No results\")\n",
        "\n",
        "    print(f\"Success querying all historical\")\n",
        "    # Turn SQlite Database into Pandas Dataframe\n",
        "    data = pd.read_sql_query(query, sqliteConnection)\n",
        "\n",
        "except:\n",
        "    sys.stderr.write(f\"Failed to select all historical\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "metadata": {
        "id": "_MxXjzzSQ1b_"
      },
      "outputs": [],
      "source": [
        "# Using the column 'time' (millisecond) add a new column 'dates' with datetime\n",
        "dates = pd.to_datetime(data['time'], unit='ms')\n",
        "tickers = data['ticker']\n",
        "dates = dates.dt.tz_localize('UTC').dt.tz_convert('US/Pacific')\n",
        "dates = dates.dt.tz_localize(None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {},
      "outputs": [],
      "source": [
        "normalized_data = pd.DataFrame(columns=['close','volume'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 388,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_by_ticker = {}\n",
        "for ticker in data['ticker'].unique():\n",
        "    data_by_ticker[ticker] = data[data['ticker'] == ticker].copy()\n",
        "    data_by_ticker[ticker]['close_norm'] = data_by_ticker[ticker]['close'] / data_by_ticker[ticker]['close'].max() #Normalized closing price data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 389,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/s4/t5945xms4x52fkpsd0m4v9340000gn/T/ipykernel_446/1044765597.py:9: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  normalized_data = pd.concat([normalized_data, temp_df], ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "for key in data_by_ticker:\n",
        "    # Create a temporary DataFrame to hold the current data\n",
        "    temp_df = pd.DataFrame({\n",
        "        'close': data_by_ticker[key]['close_norm'],\n",
        "        'volume': data_by_ticker[key]['volume']\n",
        "    })\n",
        "    \n",
        "    # Concatenate the temporary DataFrame to the normalized_data DataFrame\n",
        "    normalized_data = pd.concat([normalized_data, temp_df], ignore_index=True)\n",
        "\n",
        "# Optionally, you can reset the index if needed\n",
        "normalized_data.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MRzML9poTXB"
      },
      "source": [
        "# Train on all historical stock data, sequenced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 390,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToUrsdh6ki-s",
        "outputId": "9d50990f-8603-4c63-eb66-d56d2a54b4b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valid days: 294\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"Generate arrays filled with one-day-long sequences from the normalized dataset\n",
        "\n",
        "Arguments:\n",
        "    - data: stock dataset with 2 columns:\n",
        "        - close prices normalized (0, 1) \n",
        "        - volume\n",
        "    - dates: array of every milisecond timestamp converted to dateTime objects\n",
        "    - tickers: array of every timestamp's corresponding ticker\n",
        "\n",
        "Returns:\n",
        "    - x: array of one-day-long sequences of the normalized dataset for training\n",
        "    - y: array of one-day-long seuqences of the immediate day after for predicting\n",
        "\n",
        "\"\"\"\n",
        "def create_sequences(data, dates, tickers):\n",
        "    \n",
        "    # stores sequences to be returned\n",
        "    xs, ys = [], []\n",
        "    # index refers to the start of a day, therefore start of a 'sequence'\n",
        "    index = 0 \n",
        "     # keeps track of the number of valid sequences for debugging purposes\n",
        "    count = 0\n",
        "\n",
        "    # Loop until the end of database, stopping 2 days in advance to make room for last 'context' day and it's corresponding 'prediction' day\n",
        "    while index < len(data) - SEQUENCE_LENGTH - PREDICTION_LENGTH + 1:\n",
        "\n",
        "        # Check if sequence is within a single day (day start == day end) and (ticker start == ticker end)\n",
        "        if dates[index].date() == dates[index + SEQUENCE_LENGTH].date() and tickers[index] == tickers[index + SEQUENCE_LENGTH]:\n",
        "\n",
        "            # append current day (index -> index+SEQ) to x, and append next day (index + SEQ -> index + SEQ + PRE) to y\n",
        "            xs.append(data.iloc[index:index + SEQUENCE_LENGTH])  # Use past data for features\n",
        "            ys.append(data.iloc[index + SEQUENCE_LENGTH:index + SEQUENCE_LENGTH + PREDICTION_LENGTH, 0])  # Only predict 'close' prices\n",
        "\n",
        "            # move index to start of the next day\n",
        "            index += SEQUENCE_LENGTH\n",
        "            count += 1\n",
        "        \n",
        "        # Move index to the start of the next \n",
        "        else: # Note: This is the discarding section, can be modified to be \"imputed\" via extending the last known close value until end of day.\n",
        "\n",
        "            # move newindex to the start of the next day\n",
        "            newindex = index\n",
        "            while dates[newindex].date() == dates[newindex + 1].date():\n",
        "                newindex += 1\n",
        "            newindex += 1\n",
        "            \n",
        "            # once newindex reaches next morning, set index to match\n",
        "            index = newindex\n",
        "\n",
        "    # print the number of valid days found for debugging purposes, return arrays of sequences            \n",
        "    print(\"Valid days:\", count)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "# Create sequences from normalized data\n",
        "x, y = create_sequences(normalized_data, dates, tickers) #Creating the input and grouth truth data from create_sequences function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 391,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aidanmaldonado/Documents/PennyStock/Website/stock-prediction/venv/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 0.2224 - val_loss: 0.0374\n",
            "Epoch 2/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1513 - val_loss: 0.0434\n",
            "Epoch 3/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1271 - val_loss: 0.0456\n",
            "Epoch 4/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1051 - val_loss: 0.0409\n",
            "Epoch 5/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0926 - val_loss: 0.0358\n",
            "Epoch 6/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0749 - val_loss: 0.0428\n",
            "Epoch 7/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0754 - val_loss: 0.0444\n",
            "Epoch 8/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0636 - val_loss: 0.0385\n",
            "Epoch 9/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0567 - val_loss: 0.0348\n",
            "Epoch 10/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0656 - val_loss: 0.0552\n",
            "Epoch 11/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0611 - val_loss: 0.0287\n",
            "Epoch 12/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0681 - val_loss: 0.0689\n",
            "Epoch 13/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0632 - val_loss: 0.0308\n",
            "Epoch 14/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0585 - val_loss: 0.0451\n",
            "Epoch 15/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0585 - val_loss: 0.0533\n",
            "Epoch 16/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0591 - val_loss: 0.0616\n",
            "Epoch 17/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0578 - val_loss: 0.0302\n",
            "Epoch 18/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0549 - val_loss: 0.0484\n",
            "Epoch 19/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0508 - val_loss: 0.0372\n",
            "Epoch 20/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0610 - val_loss: 0.0484\n",
            "Epoch 21/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0565 - val_loss: 0.0331\n",
            "Epoch 22/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0568 - val_loss: 0.0458\n",
            "Epoch 23/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0598 - val_loss: 0.0328\n",
            "Epoch 24/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0519 - val_loss: 0.0601\n",
            "Epoch 25/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0562 - val_loss: 0.0363\n",
            "Epoch 26/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0609 - val_loss: 0.0415\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x374fabb20>"
            ]
          },
          "execution_count": 391,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Split data into 80% / 20% training and testing groups\n",
        "train_size = int(len(x) * 0.8)\n",
        "x_train, x_test = x[:train_size], x[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Type adjustment string -> float\n",
        "x_train = x_train.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(SEQUENCE_LENGTH, NUM_FEATURES)))\n",
        "# Need both layers because return_sequences will send its output to another LSTM layer which is required before sending to Dense layer \n",
        "model.add(LSTM(50))\n",
        " # Makes readable by NN, NN doesn't predict on sequences so it needs single dimension values\n",
        "model.add(Dense(40, activation='relu'))\n",
        "# Prevents overfitting\n",
        "model.add(Dropout(0.1))\n",
        "# Takes the results from the last LSTM layer and predicts the stock prices for PREDICTION_LENGTH steps ahead\n",
        "model.add(Dense(PREDICTION_LENGTH)) \n",
        "#Compiles the model with an adam optimizer and a mean squared error loss function\n",
        "model.compile(optimizer='adam', loss='mse') \n",
        "\n",
        "# Train the model with early stopping to prevent over fitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 393,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# Save the model weights to an external file\n",
        "model.save('model.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
