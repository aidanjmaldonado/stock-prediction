{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidanjmaldonado/penny-stock-lstm/blob/main/penny_stock.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AB9VHvt_xNXa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "import sqlite3\n",
        "import requests\n",
        "import sys\n",
        "from library.DataSetProcessor import DataSetProcessor\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJBjdbHzoWvZ"
      },
      "source": [
        "# Create database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 371,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DGRIE9XnRw8",
        "outputId": "c53b5aec-8331-490b-b5b8-37e403b087c0"
      },
      "outputs": [],
      "source": [
        "# Download database from github repository\n",
        "historical_url = \"https://raw.githubusercontent.com/CSE-115-UCSC/penny-stock-lstm/main/historicaldata.db\"\n",
        "scrape_request = requests.get(historical_url)\n",
        "\n",
        "try:\n",
        "  # Contingent on request status\n",
        "  scrape_request = requests.get(historical_url)\n",
        "  scrape_request.raise_for_status()\n",
        "\n",
        "  # Create local database from pull, name 'historicaldata.db'\n",
        "  with open(\"historical.db\", \"wb\") as db_file:\n",
        "    db_file.write(scrape_request.content)\n",
        "\n",
        "  print(\"Request to download database succeeded\")\n",
        "\n",
        "\n",
        "except:\n",
        "  # Report failed request status\n",
        "  sys.stderr.write(\"Request to download database failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KApKP8mve16y",
        "outputId": "eac13848-bd1a-4869-8752-8d266eaab896"
      },
      "outputs": [],
      "source": [
        "# Connect to SQlite database\n",
        "try:\n",
        "    db = 'historical.db'\n",
        "    sqliteConnection = sqlite3.connect(db)\n",
        "    cursor = sqliteConnection.cursor()\n",
        "    print(f'SQlite connected with {db}')\n",
        "\n",
        "except:\n",
        "    sys.stderr.write(\"Failed to connect to database\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or4POn4Ye_Lw",
        "outputId": "a222f695-d83c-4f79-bc32-348cb0749353"
      },
      "outputs": [],
      "source": [
        "# Query {ticker(s)} from Database\n",
        "try:\n",
        "    query = f\"SELECT * FROM all_historical;\"\n",
        "    cursor.execute(query)\n",
        "    if cursor.fetchone() is None:\n",
        "        raise Exception(\"No results\")\n",
        "\n",
        "    print(f\"Success querying all historical\")\n",
        "    # Turn SQlite Database into Pandas Dataframe\n",
        "    data = pd.read_sql_query(query, sqliteConnection)\n",
        "\n",
        "except:\n",
        "    sys.stderr.write(f\"Failed to select all historical\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MxXjzzSQ1b_"
      },
      "outputs": [],
      "source": [
        "# Using the column 'time' (millisecond) add a new column 'dates' with datetime\n",
        "dates = pd.to_datetime(data['time'], unit='ms')\n",
        "tickers = data['ticker']\n",
        "dates = dates.dt.tz_localize('UTC').dt.tz_convert('US/Pacific')\n",
        "dates = dates.dt.tz_localize(None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "normalized_data = pd.DataFrame(columns=['close','volume'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataByTicker = {}\n",
        "for ticker in data['ticker'].unique():\n",
        "    dataByTicker[ticker] = data[data['ticker'] == ticker].copy()\n",
        "    dataByTicker[ticker]['closeNorm'] = dataByTicker[ticker]['close'] / dataByTicker[ticker]['close'].max() #Normalized closing price data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for key in dataByTicker:\n",
        "    # Create a temporary DataFrame to hold the current data\n",
        "    temp_df = pd.DataFrame({\n",
        "        'close': dataByTicker[key]['closeNorm'],\n",
        "        'volume': dataByTicker[key]['volume']\n",
        "    })\n",
        "    \n",
        "    # Concatenate the temporary DataFrame to the normalized_data DataFrame\n",
        "    normalized_data = pd.concat([normalized_data, temp_df], ignore_index=True)\n",
        "\n",
        "# Optionally, you can reset the index if needed\n",
        "normalized_data.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MRzML9poTXB"
      },
      "source": [
        "# Train on all historical stock data, sequenced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToUrsdh6ki-s",
        "outputId": "9d50990f-8603-4c63-eb66-d56d2a54b4b5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to generate sequenced arrays based on sequence and prediction lengths\n",
        "def create_sequences(data, SEQUENCE_LENGTH, PREDICTION_LENGTH):\n",
        "    xs, ys = [], []\n",
        "    index = 0\n",
        "    count = 0\n",
        "    while index < len(data) - SEQUENCE_LENGTH - PREDICTION_LENGTH + 1:\n",
        "        # Check if sequence is within a single day\n",
        "        if dates[index].date() == dates[index + SEQUENCE_LENGTH].date() and tickers[index] == tickers[index + SEQUENCE_LENGTH]:\n",
        "            xs.append(data.iloc[index:index + SEQUENCE_LENGTH])  # Use past data for features\n",
        "            ys.append(data.iloc[index + SEQUENCE_LENGTH:index + SEQUENCE_LENGTH + PREDICTION_LENGTH, 0])  # Only predict 'close' prices\n",
        "            index += SEQUENCE_LENGTH\n",
        "            count += 1\n",
        "        else:  # Move index to the start of the next \n",
        "            # This is the discarding section, can be modified to be \"imputed\" via extending the last known close value until end of day.\n",
        "            # Fill forward\n",
        "            newindex = index\n",
        "            while dates[newindex].date() == dates[newindex + 1].date():\n",
        "                newindex += 1\n",
        "            newindex += 1\n",
        "            index = newindex\n",
        "    print(\"Valid days:\", count)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "# One day sequence length\n",
        "SEQUENCE_LENGTH = 78\n",
        "PREDICTION_LENGTH = 78\n",
        "x, y = create_sequences(normalized_data, SEQUENCE_LENGTH, PREDICTION_LENGTH) #Creating the input and grouth truth data from create_sequences function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Split data into train and test sets\n",
        "train_size = int(len(x) * 0.8) #Splitting the data into 80%-20% training and validation splits\n",
        "x_train, x_test = x[:train_size], x[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Type adjustment\n",
        "x_train = x_train.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(SEQUENCE_LENGTH, x_train.shape[2]))) \n",
        "model.add(LSTM(50)) # Need both layers because return_sequences will send its output to another LSTM layer which is required before sending to Dense layer\n",
        "model.add(Dense(40, activation='relu')) # Makes readable by NN, NN doesn't predict on sequences so it needs single dimension values\n",
        "model.add(Dropout(0.1)) # Prevents overfitting\n",
        "model.add(Dense(PREDICTION_LENGTH)) # Takes the results from the last LSTM layer and predicts the stock prices for PREDICTION_LENGTH steps ahead\n",
        "model.compile(optimizer='adam', loss='mse') #Compiles the model with an adam optimizer and a mean squared error loss function\n",
        "\n",
        "# Train the model\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n",
        "model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save('model.h5') #Saving the model weights to an external file"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
